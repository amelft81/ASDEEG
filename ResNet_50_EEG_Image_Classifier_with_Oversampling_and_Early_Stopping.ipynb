{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amelft81/ASDEEG/blob/main/ResNet_50_EEG_Image_Classifier_with_Oversampling_and_Early_Stopping.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import models, transforms\n",
        "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "import os\n",
        "import copy # For deep copying model state for early stopping\n",
        "from sklearn.model_selection import train_test_split # Import train_test_split\n",
        "\n",
        "# --- 1. Configuration Parameters ---\n",
        "# Paths to your generated synthetic dataset\n",
        "DATA_DIR = \"synthetic_eeg_dataset\"\n",
        "X_RESAMPLED_PATH = os.path.join(DATA_DIR, 'X_resampled.npy')\n",
        "Y_RESAMPLED_PATH = os.path.join(DATA_DIR, 'y_resampled.npy')\n",
        "\n",
        "# Model and training parameters\n",
        "NUM_CLASSES = 2 # ASD or NON-ASD\n",
        "BATCH_SIZE = 100 # As per paper\n",
        "LEARNING_RATE = 1e-3 # As per paper\n",
        "NUM_EPOCHS = 100 # Maximum epochs, early stopping will likely stop sooner\n",
        "PATIENCE = 10 # Number of epochs to wait for improvement before stopping (for Early Stopping)\n",
        "\n",
        "# ImageNet normalization values for pre-trained models\n",
        "# These are standard mean and std deviation for images trained on ImageNet\n",
        "# ResNet-50 expects inputs normalized with these values.\n",
        "NORM_MEAN = [0.485, 0.456, 0.406]\n",
        "NORM_STD = [0.229, 0.224, 0.225]\n",
        "\n",
        "# Set device to GPU if available, otherwise CPU\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {DEVICE}\")\n",
        "\n",
        "# --- 2. Custom Dataset Class ---\n",
        "class EEGImageDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Custom PyTorch Dataset for loading EEG images and their labels.\n",
        "    \"\"\"\n",
        "    def __init__(self, images, labels, transform=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            images (np.ndarray): NumPy array of EEG images (H, W, C).\n",
        "            labels (np.ndarray): NumPy array of corresponding labels.\n",
        "            transform (callable, optional): Optional transform to be applied on a sample.\n",
        "        \"\"\"\n",
        "        self.images = images\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Returns the total number of samples.\"\"\"\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Retrieves an image and its label at the given index.\n",
        "        Applies transformations if provided.\n",
        "        \"\"\"\n",
        "        image = self.images[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "# --- 3. Model Definition: Pre-trained ResNet-50 ---\n",
        "def get_resnet_model(num_classes, freeze_features=True):\n",
        "    \"\"\"\n",
        "    Loads a pre-trained ResNet-50 model and modifies its final layer\n",
        "    for binary classification.\n",
        "\n",
        "    Args:\n",
        "        num_classes (int): The number of output classes (2 for ASD/NON-ASD).\n",
        "        freeze_features (bool): If True, freezes all layers except the final\n",
        "                                classification layer. This is common for transfer learning.\n",
        "\n",
        "    Returns:\n",
        "        torch.nn.Module: The modified ResNet-50 model.\n",
        "    \"\"\"\n",
        "    # Load pre-trained ResNet-50\n",
        "    # The paper mentions using PyTorch model zoo and ImageNet pre-training.\n",
        "    model = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)\n",
        "    print(\"Loaded pre-trained ResNet-50 model.\")\n",
        "\n",
        "    if freeze_features:\n",
        "        # Freeze all parameters in the network\n",
        "        for param in model.parameters():\n",
        "            param.requires_grad = False\n",
        "        print(\"Frozen all feature extractor layers.\")\n",
        "\n",
        "    # Get the number of features from the original fully connected layer\n",
        "    num_ftrs = model.fc.in_features\n",
        "    # Replace the final fully connected layer with a new one for our number of classes\n",
        "    model.fc = nn.Linear(num_ftrs, num_classes)\n",
        "    print(f\"Modified final fully connected layer to output {num_classes} classes.\")\n",
        "\n",
        "    return model\n",
        "\n",
        "# --- 4. Data Loading and Preprocessing ---\n",
        "def load_and_prepare_data(x_path, y_path, batch_size, norm_mean, norm_std):\n",
        "    \"\"\"\n",
        "    Loads the dataset, applies transformations, creates DataLoaders,\n",
        "    and sets up WeightedRandomSampler for class imbalance.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        images = np.load(x_path)\n",
        "        labels = np.load(y_path)\n",
        "\n",
        "        # Convert images from uint8 (0-255) to float32 (0-1) and then normalize\n",
        "        # PyTorch expects (C, H, W) format, so we need to transpose.\n",
        "        data_transforms = transforms.Compose([\n",
        "            transforms.ToPILImage(), # Convert numpy array to PIL Image for torchvision transforms\n",
        "            transforms.ToTensor(),   # Converts PIL Image to FloatTensor (0-1) and (C, H, W)\n",
        "            transforms.Normalize(mean=norm_mean, std=norm_std) # Normalize with ImageNet stats\n",
        "        ])\n",
        "\n",
        "        # Split data into training and validation sets (e.g., 80/20 split as in paper's experiments)\n",
        "        # The paper also mentions leave-one-participant-out, which is more complex to simulate here.\n",
        "        # We'll use a standard train/val split for demonstration.\n",
        "        train_images, val_images, train_labels, val_labels = train_test_split(\n",
        "            images, labels, test_size=0.2, random_state=42, stratify=labels\n",
        "        )\n",
        "\n",
        "        print(f\"\\nTraining set distribution: {Counter(train_labels)}\")\n",
        "        print(f\"Validation set distribution: {Counter(val_labels)}\")\n",
        "\n",
        "        # Create datasets\n",
        "        train_dataset = EEGImageDataset(train_images, train_labels, transform=data_transforms)\n",
        "        val_dataset = EEGImageDataset(val_images, val_labels, transform=data_transforms)\n",
        "\n",
        "        # Implement Weighted Random Sampler for training data (as per paper)\n",
        "        # This addresses the class imbalance by giving higher probability to minority class samples.\n",
        "        class_counts = Counter(train_labels)\n",
        "        num_samples = sum(class_counts.values())\n",
        "        class_weights = {cls: num_samples / count for cls, count in class_counts.items()}\n",
        "        sample_weights = [class_weights[label] for label in train_labels]\n",
        "        sampler = WeightedRandomSampler(\n",
        "            weights=sample_weights,\n",
        "            num_samples=num_samples, # Draw 'num_samples' times (with replacement)\n",
        "            replacement=True\n",
        "        )\n",
        "        print(\"WeightedRandomSampler initialized for training data.\")\n",
        "\n",
        "        # Create DataLoaders\n",
        "        train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=sampler, num_workers=0)\n",
        "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0) # No sampler for validation\n",
        "\n",
        "        return train_loader, val_loader\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: Dataset files not found at {x_path} and {y_path}.\")\n",
        "        print(\"Please ensure you have run the 'eeg_dataset_generator' code first.\")\n",
        "        return None, None # Return None for both loaders if files are not found\n",
        "\n",
        "\n",
        "# --- 5. Training Function with Early Stopping ---\n",
        "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, patience, device):\n",
        "    \"\"\"\n",
        "    Trains the deep learning model with early stopping.\n",
        "\n",
        "    Args:\n",
        "        model (torch.nn.Module): The neural network model.\n",
        "        train_loader (DataLoader): DataLoader for the training set.\n",
        "        val_loader (DataLoader): DataLoader for the validation set.\n",
        "        criterion (torch.nn.Module): Loss function.\n",
        "        optimizer (torch.optim.Optimizer): Optimizer.\n",
        "        num_epochs (int): Maximum number of training epochs.\n",
        "        patience (int): Number of epochs to wait for validation loss improvement.\n",
        "        device (torch.device): Device to train on (CPU or GPU).\n",
        "\n",
        "    Returns:\n",
        "        torch.nn.Module: The trained model (best version based on validation loss).\n",
        "    \"\"\"\n",
        "    if train_loader is None or val_loader is None:\n",
        "        print(\"Skipping training due to data loading error.\")\n",
        "        return None\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "    epochs_no_improve = 0\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "    model.to(device) # Move model to the specified device\n",
        "\n",
        "    print(\"\\nStarting model training...\")\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train() # Set model to training mode\n",
        "        running_train_loss = 0.0\n",
        "        correct_train_predictions = 0\n",
        "        total_train_samples = 0\n",
        "\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad() # Zero the parameter gradients\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward() # Backpropagation\n",
        "            optimizer.step() # Update weights\n",
        "\n",
        "            running_train_loss += loss.item() * inputs.size(0)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total_train_samples += labels.size(0)\n",
        "            correct_train_predictions += (predicted == labels).sum().item()\n",
        "\n",
        "        epoch_train_loss = running_train_loss / total_train_samples\n",
        "        epoch_train_accuracy = correct_train_predictions / total_train_samples * 100\n",
        "\n",
        "        # --- Validation Phase ---\n",
        "        model.eval() # Set model to evaluation mode\n",
        "        running_val_loss = 0.0\n",
        "        correct_val_predictions = 0\n",
        "        total_val_samples = 0\n",
        "\n",
        "        with torch.no_grad(): # Disable gradient calculation for validation\n",
        "            for inputs, labels in val_loader:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                running_val_loss += loss.item() * inputs.size(0)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total_val_samples += labels.size(0)\n",
        "                correct_val_predictions += (predicted == labels).sum().item()\n",
        "\n",
        "        epoch_val_loss = running_val_loss / total_val_samples\n",
        "        epoch_val_accuracy = correct_val_predictions / total_val_samples * 100\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}: \"\n",
        "              f\"Train Loss: {epoch_train_loss:.4f}, Train Acc: {epoch_train_accuracy:.2f}% | \"\n",
        "              f\"Val Loss: {epoch_val_loss:.4f}, Val Acc: {epoch_val_accuracy:.2f}%\")\n",
        "\n",
        "        # --- Early Stopping Logic ---\n",
        "        if epoch_val_loss < best_val_loss:\n",
        "            best_val_loss = epoch_val_loss\n",
        "            best_model_wts = copy.deepcopy(model.state_dict()) # Save the best model state\n",
        "            epochs_no_improve = 0\n",
        "            print(f\"  Validation loss improved. Saving model state. Best Loss: {best_val_loss:.4f}\")\n",
        "        else:\n",
        "            epochs_no_improve += 1\n",
        "            print(f\"  Validation loss did not improve. Patience: {epochs_no_improve}/{patience}\")\n",
        "            if epochs_no_improve >= patience:\n",
        "                print(f\"Early stopping triggered after {epoch+1} epochs due to no improvement in validation loss.\")\n",
        "                model.load_state_dict(best_model_wts) # Load the best model weights\n",
        "                return model\n",
        "\n",
        "    print(\"Training finished (max epochs reached).\")\n",
        "    model.load_state_dict(best_model_wts) # Load the best model weights even if max epochs reached\n",
        "    return model\n",
        "\n",
        "# --- Main Execution ---\n",
        "if __name__ == \"__main__\":\n",
        "    # 1. Load and prepare data\n",
        "    train_loader, val_loader = load_and_prepare_data(\n",
        "        X_RESAMPLED_PATH, Y_RESAMPLED_PATH, BATCH_SIZE, NORM_MEAN, NORM_STD\n",
        "    )\n",
        "\n",
        "    # 2. Get the ResNet-50 model\n",
        "    model = get_resnet_model(NUM_CLASSES, freeze_features=True) # Freeze features for faster initial training\n",
        "\n",
        "    # 3. Define Loss Function and Optimizer\n",
        "    # CrossEntropyLoss is suitable for multi-class classification (even binary)\n",
        "    # and implicitly applies softmax.\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    # Adam optimizer as specified in the paper\n",
        "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "    # 4. Train the model\n",
        "    trained_model = train_model(\n",
        "        model, train_loader, val_loader, criterion, optimizer, NUM_EPOCHS, PATIENCE, DEVICE\n",
        "    )\n",
        "\n",
        "    print(\"\\nModel training complete. The 'trained_model' variable holds the best model weights.\")\n",
        "    # You can now save the trained model for future inference:\n",
        "    # torch.save(trained_model.state_dict(), 'resnet50_eeg_classifier.pth')\n",
        "    # To load:\n",
        "    # model = get_resnet_model(NUM_CLASSES)\n",
        "    # model.load_state_dict(torch.load('resnet50_eeg_classifier.pth'))\n",
        "    # model.eval() # Set to evaluation mode before inference"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "Error: Dataset files not found at synthetic_eeg_dataset/X_resampled.npy and synthetic_eeg_dataset/y_resampled.npy.\n",
            "Please ensure you have run the 'eeg_dataset_generator' code first.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
            "100%|██████████| 97.8M/97.8M [00:00<00:00, 160MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded pre-trained ResNet-50 model.\n",
            "Frozen all feature extractor layers.\n",
            "Modified final fully connected layer to output 2 classes.\n",
            "Skipping training due to data loading error.\n",
            "\n",
            "Model training complete. The 'trained_model' variable holds the best model weights.\n"
          ]
        }
      ],
      "execution_count": 1,
      "metadata": {
        "id": "fK3wQ5BZvbI9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93b5b038-dace-4833-be99-3539f14cd80e"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}