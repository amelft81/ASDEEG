{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amelft81/ASDEEG/blob/main/Complete_EEG_to_ASD_Prediction_Pipeline_(Simulated_Data).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import models, transforms\n",
        "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "import os\n",
        "import copy # For deep copying model state for early stopping\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.interpolate import griddata\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "\n",
        "# --- 1. Configuration Parameters ---\n",
        "# Data generation parameters (from eeg_dataset_generator)\n",
        "NUM_ELECTRODES = 116 # Paper mentions 116 electrodes after exclusion\n",
        "SAMPLING_RATE = 500  # Hz, as per paper\n",
        "SEGMENT_LENGTH_SEC = 1 # seconds, as per paper\n",
        "NUM_SAMPLES_PER_SEGMENT = SAMPLING_RATE * SEGMENT_LENGTH_SEC # 500 samples\n",
        "IMAGE_SIZE = 224     # 224x224 pixels, as per ResNet-50 input\n",
        "FREQ_BANDS = {\n",
        "    'theta': (4, 7),   # Hz\n",
        "    'alpha': (8, 13),  # Hz\n",
        "    'beta': (13, 30)   # Hz\n",
        "}\n",
        "# Desired initial class distribution (approximate from paper: 81% NON-ASD, 19% ASD)\n",
        "ASD_RATIO = 0.19\n",
        "INITIAL_TOTAL_SAMPLES = 1000 # Total samples to generate before oversampling (for demonstration)\n",
        "\n",
        "# Model and training parameters (from resnet_eeg_classifier)\n",
        "NUM_CLASSES = 2 # ASD (1) or NON-ASD (0)\n",
        "BATCH_SIZE = 100 # As per paper\n",
        "LEARNING_RATE = 1e-3 # As per paper\n",
        "NUM_EPOCHS = 100 # Maximum epochs, early stopping will likely stop sooner\n",
        "PATIENCE = 10 # Number of epochs to wait for improvement before stopping (for Early Stopping)\n",
        "\n",
        "# ImageNet normalization values for pre-trained models\n",
        "NORM_MEAN = [0.485, 0.456, 0.406]\n",
        "NORM_STD = [0.229, 0.224, 0.225]\n",
        "\n",
        "# Set device to GPU if available, otherwise CPU\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {DEVICE}\")\n",
        "\n",
        "# --- 2. Simulated EEG Data Generation and Image Transformation Functions ---\n",
        "\n",
        "def generate_electrode_positions(num_electrodes, radius=0.5):\n",
        "    \"\"\"Generates simulated 2D electrode positions in a circular pattern.\"\"\"\n",
        "    angles = np.linspace(0, 2 * np.pi, num_electrodes, endpoint=False)\n",
        "    x = radius * np.cos(angles) + np.random.normal(0, 0.05, num_electrodes) # Add some noise\n",
        "    y = radius * np.sin(angles) + np.random.normal(0, 0.05, num_electrodes) # Add some noise\n",
        "    # Add a central electrode\n",
        "    x = np.append(x, 0)\n",
        "    y = np.append(y, 0)\n",
        "    return np.array([x, y]).T\n",
        "\n",
        "def generate_eeg_signal(num_samples, sampling_rate, freq_bands, is_asd=False):\n",
        "    \"\"\"\n",
        "    Generates a simulated EEG signal for one electrode.\n",
        "    ASD signals might have slightly different characteristics (e.g., more noise or altered band power).\n",
        "    \"\"\"\n",
        "    t = np.linspace(0, num_samples / sampling_rate, num_samples, endpoint=False)\n",
        "    signal = np.zeros(num_samples)\n",
        "\n",
        "    # Add base frequencies for each band\n",
        "    for band_name, (low_f, high_f) in freq_bands.items():\n",
        "        center_f = (low_f + high_f) / 2\n",
        "        amplitude = np.random.uniform(0.5, 1.5)\n",
        "        # Simplified simulation: ASD signals might have altered power in certain bands\n",
        "        if is_asd:\n",
        "            if band_name == 'theta': # Example: slightly higher theta in ASD\n",
        "                amplitude *= 1.2\n",
        "            elif band_name == 'beta': # Example: slightly lower beta in ASD\n",
        "                amplitude *= 0.8\n",
        "        signal += amplitude * np.sin(2 * np.pi * center_f * t + np.random.uniform(0, 2 * np.pi))\n",
        "\n",
        "    # Add random noise\n",
        "    noise_level = np.random.uniform(0.1, 0.5)\n",
        "    if is_asd: # Simplified simulation: ASD signals might be noisier\n",
        "        noise_level *= 1.2\n",
        "    signal += noise_level * np.random.randn(num_samples)\n",
        "\n",
        "    return signal\n",
        "\n",
        "def get_band_power(signal, sampling_rate, freq_range):\n",
        "    \"\"\"Calculates the average power in a specific frequency range using FFT.\"\"\"\n",
        "    n = len(signal)\n",
        "    yf = np.fft.fft(signal)\n",
        "    xf = np.fft.fftfreq(n, 1 / sampling_rate)\n",
        "\n",
        "    # Find indices corresponding to the frequency range\n",
        "    min_freq, max_freq = freq_range\n",
        "    indices = np.where((xf >= min_freq) & (xf <= max_freq))\n",
        "\n",
        "    # Calculate power (magnitude squared) in the specified band\n",
        "    power = np.mean(np.abs(yf[indices])**2)\n",
        "    return power\n",
        "\n",
        "def normalize_band_to_uint8(arr):\n",
        "    \"\"\"\n",
        "    Normalizes a 2D array to the 0-255 range and converts it to uint8.\n",
        "    Handles cases where min/max might be the same (flat array).\n",
        "    \"\"\"\n",
        "    min_val = np.min(arr)\n",
        "    max_val = np.max(arr)\n",
        "    if max_val == min_val:\n",
        "        return np.full(arr.shape, 128, dtype=np.uint8) # Default grey if flat\n",
        "    return ((arr - min_val) / (max_val - min_val) * 255).astype(np.uint8)\n",
        "\n",
        "def create_eeg_image(electrode_positions, band_powers, image_size):\n",
        "    \"\"\"\n",
        "    Interpolates band powers onto a 2D grid and creates an RGB image.\n",
        "    Each channel (R, G, B) corresponds to a frequency band.\n",
        "    \"\"\"\n",
        "    grid_x, grid_y = np.mgrid[-1:1:complex(0, image_size), -1:1:complex(0, image_size)]\n",
        "\n",
        "    # Interpolate for each band\n",
        "    # Method='cubic' for smoother interpolation, fill_value=0 for outside points\n",
        "    interp_theta = griddata(electrode_positions, band_powers['theta'], (grid_x, grid_y), method='cubic', fill_value=0)\n",
        "    interp_alpha = griddata(electrode_positions, band_powers['alpha'], (grid_x, grid_y), method='cubic', fill_value=0)\n",
        "    interp_beta = griddata(electrode_positions, band_powers['beta'], (grid_x, grid_y), method='cubic', fill_value=0)\n",
        "\n",
        "    # Assign bands to RGB channels and normalize\n",
        "    # Theta -> Red, Alpha -> Green, Beta -> Blue (common choice, can be varied)\n",
        "    img_r = normalize_band_to_uint8(interp_theta)\n",
        "    img_g = normalize_band_to_uint8(interp_alpha)\n",
        "    img_b = normalize_band_to_uint8(interp_beta)\n",
        "\n",
        "    # Stack into an RGB image (H, W, C)\n",
        "    eeg_image = np.stack([img_r, img_g, img_b], axis=-1)\n",
        "    return eeg_image\n",
        "\n",
        "def generate_and_oversample_dataset(num_samples_total, asd_ratio, image_size, num_electrodes, sampling_rate, freq_bands):\n",
        "    \"\"\"\n",
        "    Generates a synthetic EEG image dataset and then oversamples the minority class.\n",
        "    Returns X (images) and y (labels) as numpy arrays.\n",
        "    \"\"\"\n",
        "    dataset = []\n",
        "    electrode_positions = generate_electrode_positions(num_electrodes)\n",
        "\n",
        "    print(f\"Generating {num_samples_total} initial synthetic EEG images...\")\n",
        "    for i in range(num_samples_total):\n",
        "        is_asd = np.random.rand() < asd_ratio\n",
        "        label = 1 if is_asd else 0 # 1 for ASD, 0 for NON-ASD\n",
        "\n",
        "        # Generate EEG signals for all electrodes\n",
        "        # +1 because generate_electrode_positions adds a central electrode\n",
        "        electrode_signals = [generate_eeg_signal(NUM_SAMPLES_PER_SEGMENT, sampling_rate, freq_bands, is_asd)\n",
        "                             for _ in range(num_electrodes + 1)]\n",
        "\n",
        "        # Calculate band powers for each electrode\n",
        "        band_powers_per_electrode = {band: [] for band in freq_bands}\n",
        "        for signal in electrode_signals:\n",
        "            for band_name, freq_range in freq_bands.items():\n",
        "                power = get_band_power(signal, sampling_rate, freq_range)\n",
        "                band_powers_per_electrode[band_name].append(power)\n",
        "\n",
        "        # Create the EEG image\n",
        "        eeg_image = create_eeg_image(electrode_positions, band_powers_per_electrode, image_size)\n",
        "        dataset.append((eeg_image, label))\n",
        "\n",
        "        if (i + 1) % (num_samples_total // 10) == 0:\n",
        "            print(f\"  Generated {i + 1}/{num_samples_total} samples...\")\n",
        "\n",
        "    print(\"Initial dataset generation complete.\")\n",
        "\n",
        "    X = np.array([item[0] for item in dataset])\n",
        "    y = np.array([item[1] for item in dataset])\n",
        "\n",
        "    print(f\"Original dataset shape: {Counter(y)}\")\n",
        "\n",
        "    # Apply RandomOverSampler to balance the dataset\n",
        "    print(\"Applying RandomOverSampler to balance the dataset...\")\n",
        "    n_samples, h, w, c = X.shape\n",
        "    X_flat = X.reshape(n_samples, -1) # Flatten images for the sampler\n",
        "\n",
        "    from imblearn.over_sampling import RandomOverSampler\n",
        "    ros = RandomOverSampler(random_state=42)\n",
        "    X_resampled_flat, y_resampled = ros.fit_resample(X_flat, y)\n",
        "\n",
        "    X_resampled = X_resampled_flat.reshape(-1, h, w, c) # Reshape images back\n",
        "    print(f\"Resampled dataset shape: {Counter(y_resampled)}\")\n",
        "    print(\"Oversampling complete.\")\n",
        "\n",
        "    return X_resampled, y_resampled\n",
        "\n",
        "# --- 3. Custom PyTorch Dataset Class ---\n",
        "class EEGImageDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Custom PyTorch Dataset for loading EEG images and their labels.\n",
        "    \"\"\"\n",
        "    def __init__(self, images, labels, transform=None):\n",
        "        self.images = images\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = self.images[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "# --- 4. Model Definition: Pre-trained ResNet-50 ---\n",
        "def get_resnet_model(num_classes, freeze_features=True):\n",
        "    \"\"\"\n",
        "    Loads a pre-trained ResNet-50 model and modifies its final layer\n",
        "    for binary classification.\n",
        "    \"\"\"\n",
        "    model = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)\n",
        "    print(\"Loaded pre-trained ResNet-50 model.\")\n",
        "\n",
        "    if freeze_features:\n",
        "        for param in model.parameters():\n",
        "            param.requires_grad = False\n",
        "        print(\"Frozen all feature extractor layers.\")\n",
        "\n",
        "    num_ftrs = model.fc.in_features\n",
        "    model.fc = nn.Linear(num_ftrs, num_classes)\n",
        "    print(f\"Modified final fully connected layer to output {num_classes} classes.\")\n",
        "\n",
        "    return model\n",
        "\n",
        "# --- 5. Data Loading and Preprocessing (using generated data) ---\n",
        "def prepare_data_loaders(images, labels, batch_size, norm_mean, norm_std):\n",
        "    \"\"\"\n",
        "    Prepares PyTorch DataLoaders from the generated and oversampled dataset.\n",
        "    \"\"\"\n",
        "    data_transforms = transforms.Compose([\n",
        "        transforms.ToPILImage(), # Convert numpy array to PIL Image for torchvision transforms\n",
        "        transforms.ToTensor(),   # Converts PIL Image to FloatTensor (0-1) and (C, H, W)\n",
        "        transforms.Normalize(mean=norm_mean, std=norm_std) # Normalize with ImageNet stats\n",
        "    ])\n",
        "\n",
        "    # Split data into training and validation sets (80/20 split as in paper's experiments)\n",
        "    train_images, val_images, train_labels, val_labels = train_test_split(\n",
        "        images, labels, test_size=0.2, random_state=42, stratify=labels\n",
        "    )\n",
        "\n",
        "    print(f\"\\nTraining set distribution after split: {Counter(train_labels)}\")\n",
        "    print(f\"Validation set distribution after split: {Counter(val_labels)}\")\n",
        "\n",
        "    # Create datasets\n",
        "    train_dataset = EEGImageDataset(train_images, train_labels, transform=data_transforms)\n",
        "    val_dataset = EEGImageDataset(val_images, val_labels, transform=data_transforms)\n",
        "\n",
        "    # Weighted Random Sampler for training data (already oversampled, but sampler ensures balanced batches)\n",
        "    # The oversampling already balanced the dataset, so weights here will be uniform if `num_samples` is the new total.\n",
        "    # However, the paper explicitly mentions WRS with replacement for balanced mini-batches,\n",
        "    # so we'll re-calculate weights based on the (now balanced) train_labels.\n",
        "    class_counts = Counter(train_labels)\n",
        "    num_samples_train = sum(class_counts.values())\n",
        "    class_weights = {cls: num_samples_train / count for cls, count in class_counts.items()}\n",
        "    sample_weights = [class_weights[label] for label in train_labels]\n",
        "    sampler = WeightedRandomSampler(\n",
        "        weights=sample_weights,\n",
        "        num_samples=num_samples_train, # Draw 'num_samples_train' times (with replacement)\n",
        "        replacement=True\n",
        "    )\n",
        "    print(\"WeightedRandomSampler initialized for training data loaders.\")\n",
        "\n",
        "    # Create DataLoaders\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=sampler, num_workers=0)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
        "\n",
        "    return train_loader, val_loader\n",
        "\n",
        "# --- 6. Training Function with Early Stopping ---\n",
        "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, patience, device):\n",
        "    \"\"\"\n",
        "    Trains the deep learning model with early stopping.\n",
        "    \"\"\"\n",
        "    best_val_loss = float('inf')\n",
        "    epochs_no_improve = 0\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "    model.to(device)\n",
        "\n",
        "    print(\"\\nStarting model training...\")\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_train_loss = 0.0\n",
        "        correct_train_predictions = 0\n",
        "        total_train_samples = 0\n",
        "\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_train_loss += loss.item() * inputs.size(0)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total_train_samples += labels.size(0)\n",
        "            correct_train_predictions += (predicted == labels).sum().item()\n",
        "\n",
        "        epoch_train_loss = running_train_loss / total_train_samples\n",
        "        epoch_train_accuracy = correct_train_predictions / total_train_samples * 100\n",
        "\n",
        "        # --- Validation Phase ---\n",
        "        model.eval()\n",
        "        running_val_loss = 0.0\n",
        "        correct_val_predictions = 0\n",
        "        total_val_samples = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                running_val_loss += loss.item() * inputs.size(0)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total_val_samples += labels.size(0)\n",
        "                correct_val_predictions += (predicted == labels).sum().item()\n",
        "\n",
        "        epoch_val_loss = running_val_loss / total_val_samples\n",
        "        epoch_val_accuracy = correct_val_predictions / total_val_samples * 100\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}: \"\n",
        "              f\"Train Loss: {epoch_train_loss:.4f}, Train Acc: {epoch_train_accuracy:.2f}% | \"\n",
        "              f\"Val Loss: {epoch_val_loss:.4f}, Val Acc: {epoch_val_accuracy:.2f}%\")\n",
        "\n",
        "        # --- Early Stopping Logic ---\n",
        "        if epoch_val_loss < best_val_loss:\n",
        "            best_val_loss = epoch_val_loss\n",
        "            best_model_wts = copy.deepcopy(model.state_dict())\n",
        "            epochs_no_improve = 0\n",
        "            # print(f\"  Validation loss improved. Saving model state. Best Loss: {best_val_loss:.4f}\") # Uncomment for more verbose output\n",
        "        else:\n",
        "            epochs_no_improve += 1\n",
        "            # print(f\"  Validation loss did not improve. Patience: {epochs_no_improve}/{patience}\") # Uncomment for more verbose output\n",
        "            if epochs_no_improve >= patience:\n",
        "                print(f\"Early stopping triggered after {epoch+1} epochs due to no improvement in validation loss.\")\n",
        "                model.load_state_dict(best_model_wts)\n",
        "                return model\n",
        "\n",
        "    print(\"Training finished (max epochs reached).\")\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model\n",
        "\n",
        "# --- 7. Evaluation Function ---\n",
        "def evaluate_model(model, data_loader, device, dataset_name=\"Test\"):\n",
        "    \"\"\"\n",
        "    Evaluates the model's performance on a given DataLoader.\n",
        "    \"\"\"\n",
        "    model.eval() # Set model to evaluation mode\n",
        "    all_labels = []\n",
        "    all_predictions = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in data_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_predictions.extend(predicted.cpu().numpy())\n",
        "\n",
        "    accuracy = accuracy_score(all_labels, all_predictions)\n",
        "    precision = precision_score(all_labels, all_predictions, average='binary') # 'binary' for 2 classes\n",
        "    recall = recall_score(all_labels, all_predictions, average='binary')\n",
        "    f1 = f1_score(all_labels, all_predictions, average='binary')\n",
        "    cm = confusion_matrix(all_labels, all_predictions)\n",
        "\n",
        "    print(f\"\\n--- {dataset_name} Set Evaluation ---\")\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall: {recall:.4f}\")\n",
        "    print(f\"F1-Score: {f1:.4f}\")\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(cm)\n",
        "    print(f\"  (Rows: True Labels, Columns: Predicted Labels)\")\n",
        "    print(f\"  [[True Negative (NON-ASD predicted NON-ASD), False Positive (NON-ASD predicted ASD)]\")\n",
        "    print(f\"   [False Negative (ASD predicted NON-ASD), True Positive (ASD predicted ASD)]]\")\n",
        "\n",
        "\n",
        "# --- Main Execution ---\n",
        "if __name__ == \"__main__\":\n",
        "    # 1. Generate and Oversample the Dataset\n",
        "    X_resampled, y_resampled = generate_and_oversample_dataset(\n",
        "        num_samples_total=INITIAL_TOTAL_SAMPLES,\n",
        "        asd_ratio=ASD_RATIO,\n",
        "        image_size=IMAGE_SIZE,\n",
        "        num_electrodes=NUM_ELECTRODES,\n",
        "        sampling_rate=SAMPLING_RATE,\n",
        "        freq_bands=FREQ_BANDS\n",
        "    )\n",
        "\n",
        "    # 2. Prepare DataLoaders for training and validation\n",
        "    train_loader, val_loader = prepare_data_loaders(\n",
        "        X_resampled, y_resampled, BATCH_SIZE, NORM_MEAN, NORM_STD\n",
        "    )\n",
        "\n",
        "    # 3. Get the ResNet-50 model\n",
        "    model = get_resnet_model(NUM_CLASSES, freeze_features=True)\n",
        "\n",
        "    # 4. Define Loss Function and Optimizer\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "    # 5. Train the model\n",
        "    trained_model = train_model(\n",
        "        model, train_loader, val_loader, criterion, optimizer, NUM_EPOCHS, PATIENCE, DEVICE\n",
        "    )\n",
        "\n",
        "    print(\"\\n--- Training Complete ---\")\n",
        "\n",
        "    # 6. Evaluate the trained model on the validation set\n",
        "    evaluate_model(trained_model, val_loader, DEVICE, dataset_name=\"Validation\")\n",
        "\n",
        "    # Optional: Visualize a few generated images before training\n",
        "    # This part is included in the eeg_dataset_generator artifact for initial checks.\n",
        "    # You can uncomment and run it here if you want to see samples again.\n",
        "    # print(\"\\nDisplaying a few generated EEG images from the resampled dataset...\")\n",
        "    # fig, axes = plt.subplots(2, 4, figsize=(12, 6))\n",
        "    # axes = axes.flatten()\n",
        "    # for i in range(min(8, len(X_resampled))):\n",
        "    #     ax = axes[i]\n",
        "    #     ax.imshow(X_resampled[i])\n",
        "    #     ax.set_title(f\"Label: {'ASD' if y_resampled[i] == 1 else 'NON-ASD'}\")\n",
        "    #     ax.axis('off')\n",
        "    # plt.tight_layout()\n",
        "    # plt.show()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "ZhLs0oV3x50i"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}